{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_extlist(path_dataset, extension='.txt'):\n",
    "    file_list = []\n",
    "    for (path, dir, files) in os.walk(path_dataset):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "            if ext == extension:\n",
    "                file_list.append(path + \"/\" + filename)\n",
    "\n",
    "    return file_list\n",
    "\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "\n",
    "from scipy import io\n",
    "def mat2txt(mat_path, scene_path):\n",
    "    mat_file = io.loadmat(mat_path)\n",
    "    file_name = mat_path.split('/')[-1]\n",
    "    file_name = os.path.splitext(file_name)[0] + '.txt'\n",
    "    f = open(scene_path + '/' + file_name, 'w')\n",
    "    for point in mat_file['image_info'][0][0][0][0][0]:\n",
    "        contents = str(point[0]) + ' ' + str(point[1]) + '\\n'\n",
    "        f.writelines(contents)\n",
    "    f.close()\n",
    "\n",
    "import json\n",
    "def json2txt(json_path, save_path):\n",
    "    with open(json_path, \"r\") as file:\n",
    "        json_file = json.load(file)\n",
    "    txt_name = json_path.split('/')[-1]\n",
    "    txt_name = os.path.splitext(txt_name)[0] + '.txt'\n",
    "    f = open(save_path + '/' + txt_name, 'w')\n",
    "    points = json_file['points']\n",
    "    for point in points:\n",
    "        contents = str(point[0]) + ' '+ str(point[1]) + '\\n'\n",
    "        f.writelines(contents)\n",
    "    f.close()   \n",
    "     \n",
    "def makelist(source_path):\n",
    "    img_path_list = find_extlist(source_path, '.jpg')\n",
    "    spl = '/'.join([source_path.split('/')[-3], source_path.split('/')[-2]]) + '/'\n",
    "    file_path = source_path + '/../' + source_path.split('/')[-1] + '.list'\n",
    "    file = open(file_path, 'w')\n",
    "    for img_path in img_path_list:\n",
    "        img_path = img_path.split(spl)[-1]\n",
    "        mat_path = os.path.splitext(img_path)[0] + '.txt'\n",
    "        contents = img_path + ' ' + mat_path + '\\n'\n",
    "        file.writelines(contents)\n",
    "    file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd'\n",
    "txt_path = path + '/annotation_txt'\n",
    "createFolder(txt_path)\n",
    "json_list = find_extlist(path, '.json')\n",
    "\n",
    "for file in json_list:\n",
    "    json2txt(file, txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "file = open('/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/train.txt', 'r')\n",
    "lines = file.read().split('\\n')\n",
    "file.close()\n",
    "\n",
    "for idx, line in enumerate(lines):\n",
    "    img_file = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/images/' + line.split(' ')[0] + '.jpg'\n",
    "    if line.split(' ')[0] == '':\n",
    "        continue\n",
    "    txt_file = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/annotation_txt/' + line.split(' ')[0] + '.txt'\n",
    "    img = cv2.imread(img_file)\n",
    "    h, w, c =img.shape\n",
    "    ratio_h = 1080. / h\n",
    "    ratio_w = 1920. / w\n",
    "    r = min(ratio_w, ratio_h)\n",
    "    new_shape_h = int(h * r)\n",
    "    new_shape_w = int(w * r)\n",
    "    \n",
    "    if r < 1.:\n",
    "        resize_img = cv2.resize(img, (new_shape_w, new_shape_h), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        resize_img = cv2.resize(img, (new_shape_w, new_shape_h), interpolation=cv2.INTER_LINEAR)\n",
    "    os.mkdir('/usr/src/app/CrowdCounting-P2PNet/datasets/ver4/train/scene'+str(idx+1))\n",
    "    cv2.imwrite('/usr/src/app/CrowdCounting-P2PNet/datasets/ver4/train/scene' + str(idx+1) + '/IMG_' + str(idx+1) + '.jpg', resize_img)\n",
    "    file = open(txt_file, 'r')\n",
    "    txt_lines = file.read().split('\\n')\n",
    "    file.close()\n",
    "    contents = ''\n",
    "    for txt_line in txt_lines:\n",
    "        if txt_line == '':\n",
    "            break\n",
    "        try:\n",
    "            txt_line = txt_line.split(' ')\n",
    "            contents += str(float(txt_line[0]) * r) + ' ' + str(float(txt_line[1]) * r) + '\\n'\n",
    "        except Exception as e:\n",
    "            print(txt_line)\n",
    "    file = open('/usr/src/app/CrowdCounting-P2PNet/datasets/ver4/train/scene' + str(idx+1) + '/GT_IMG_' + str(idx+1) + '.txt', 'w')\n",
    "    file.writelines(contents)\n",
    "    file.close()\n",
    "makelist('/usr/src/app/CrowdCounting-P2PNet/datasets/ver4/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/val.txt', 'r')\n",
    "lines = file.read().split('\\n')\n",
    "file.close()\n",
    "\n",
    "for idx, line in enumerate(lines):\n",
    "    if line.split(' ')[0] == '':\n",
    "        continue\n",
    "    img_file = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/images/' + line.split(' ')[0] + '.jpg'\n",
    "    txt_file = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/annotation_txt/' + line.split(' ')[0] + '.txt'\n",
    "    img = cv2.imread(img_file)\n",
    "    h, w, c = img.shape\n",
    "\n",
    "    ratio_h = 1080. / h\n",
    "    ratio_w = 1920. / w\n",
    "    r = min(ratio_w, ratio_h)\n",
    "    new_shape_h = int(h * r)\n",
    "    new_shape_w = int(w * r)\n",
    "    \n",
    "    if r < 1.:\n",
    "        resize_img = cv2.resize(img, (new_shape_w, new_shape_h), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        resize_img = cv2.resize(img, (new_shape_w, new_shape_h), interpolation=cv2.INTER_LINEAR)\n",
    "    os.mkdir('/usr/src/app/CrowdCounting-P2PNet/datasets/ver4/test/scene'+str(idx+1))\n",
    "    cv2.imwrite('/usr/src/app/CrowdCounting-P2PNet/datasets/ver4/test/scene' + str(idx+1) + '/IMG_' + str(idx+1) + '.jpg', resize_img)\n",
    "    file = open(txt_file, 'r')\n",
    "    txt_lines = file.read().split('\\n')\n",
    "    file.close()\n",
    "    contents = ''\n",
    "    for txt_line in txt_lines:\n",
    "        if txt_line == '':\n",
    "            break\n",
    "        try:\n",
    "            txt_line = txt_line.split(' ')\n",
    "            contents += str(float(txt_line[0]) * r) + ' ' + str(float(txt_line[1]) * r) + '\\n'\n",
    "        except Exception as e:\n",
    "            print(txt_line)\n",
    "\n",
    "    file = open('/usr/src/app/CrowdCounting-P2PNet/datasets/ver4/test/scene' + str(idx+1) + '/GT_IMG_' + str(idx+1) + '.txt', 'w')\n",
    "    file.writelines(contents)\n",
    "    file.close()\n",
    "makelist('/usr/src/app/CrowdCounting-P2PNet/datasets/ver4/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makelist('/usr/src/app/CrowdCounting-P2PNet/datasets/ver1/train')\n",
    "makelist('/usr/src/app/CrowdCounting-P2PNet/datasets/ver1/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def gen_discrete_map(im_height, im_width, points):\n",
    "    \"\"\"\n",
    "        func: generate the discrete map.\n",
    "        points: [num_gt, 2], for each row: [width, height]\n",
    "        \"\"\"\n",
    "    discrete_map = np.zeros([im_height, im_width], dtype=np.float32)\n",
    "    h, w = discrete_map.shape[:2]\n",
    "    num_gt = points.shape[0]\n",
    "    if num_gt == 0:\n",
    "        return discrete_map\n",
    "    \n",
    "    # fast create discrete map\n",
    "    points_np = np.array(points).round().astype(int)\n",
    "    p_h = np.minimum(points_np[:, 1], np.array([h-1]*num_gt).astype(int))\n",
    "    p_w = np.minimum(points_np[:, 0], np.array([w-1]*num_gt).astype(int))\n",
    "    p_index = torch.from_numpy(p_h* im_width + p_w)\n",
    "    discrete_map = torch.zeros(im_width * im_height).scatter_add_(0, index=p_index, src=torch.ones(im_width*im_height)).view(im_height, im_width).numpy()\n",
    "\n",
    "    ''' slow method\n",
    "    for p in points:\n",
    "        p = np.round(p).astype(int)\n",
    "        p[0], p[1] = min(h - 1, p[1]), min(w - 1, p[0])\n",
    "        discrete_map[p[0], p[1]] += 1\n",
    "    '''\n",
    "    assert np.sum(discrete_map) == num_gt\n",
    "    \n",
    "    return discrete_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def json2yolotxt(json_path, save_path):\n",
    "    with open(json_path, \"r\") as file:\n",
    "        json_file = json.load(file)\n",
    "    txt_name = json_path.split('/')[-1]\n",
    "    txt_name = os.path.splitext(txt_name)[0] + '.txt'\n",
    "    f = open(save_path + '/' + txt_name, 'w')\n",
    "    boxes = json_file[\"boxes\"]\n",
    "    for box in boxes:\n",
    "        # x1, y1, x2, y2\n",
    "        contents = str(box[0]) + ' '+ str(box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + '\\n'\n",
    "        f.writelines(contents)\n",
    "    f.close()   \n",
    "\n",
    "path = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd'\n",
    "txt_path = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/annotation_box_txt'\n",
    "createFolder(txt_path)\n",
    "json_list = find_extlist(path, '.json')\n",
    "\n",
    "for file in json_list:\n",
    "    json2yolotxt(file, txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid SOS parameters for sequential JPEG\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import random\n",
    "import os\n",
    "txt_list = find_extlist('/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/annotation_box_txt', '.txt')\n",
    "save_img_path = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/images'\n",
    "save_txt_path = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/labels'\n",
    "\n",
    "seq = list(range(len(txt_list)))\n",
    "random.shuffle(seq)\n",
    "\n",
    "for idx, value in enumerate(seq):\n",
    "    if idx > (len(seq) * 0.8):\n",
    "        train = False\n",
    "    else:\n",
    "        train = True\n",
    "\n",
    "    img_file = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/images/' + os.path.splitext(txt_list[value].split('/')[-1])[0] + '.jpg'\n",
    "    img = cv2.imread(img_file)\n",
    "    h, w, c = img.shape\n",
    "\n",
    "    ratio_h = 1280. / h\n",
    "    ratio_w = 1280. / w\n",
    "    r = min(ratio_w, ratio_h)\n",
    "    new_shape_h = int(h * r)\n",
    "    new_shape_w = int(w * r)\n",
    "    \n",
    "    if r < 1.:\n",
    "        resize_img = cv2.resize(img, (new_shape_w, new_shape_h), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        resize_img = cv2.resize(img, (new_shape_w, new_shape_h), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    new_img_file = img_file.split('/')[-1]\n",
    "    if train:\n",
    "        cv2.imwrite('/usr/src/app/CrowdCounting-P2PNet/yolo_NWPU/images/train/' + new_img_file, resize_img)\n",
    "    else:\n",
    "        cv2.imwrite('/usr/src/app/CrowdCounting-P2PNet/yolo_NWPU/images/val/' + new_img_file, resize_img)\n",
    "    \n",
    "    txt_file = '/usr/src/app/CrowdCounting-P2PNet/NWPUCrowd/annotation_box_txt/' + os.path.splitext(new_img_file)[0] + '.txt'\n",
    "    new_txt_file = os.path.splitext(new_img_file)[0] + '.txt'\n",
    "\n",
    "    file = open(txt_file, 'r')\n",
    "    txt_lines = file.read().split('\\n')\n",
    "    file.close()\n",
    "    contents = ''\n",
    "\n",
    "    for txt_line in txt_lines:\n",
    "        if txt_line == '':\n",
    "            break\n",
    "        try:\n",
    "            txt_line = txt_line.split(' ')\n",
    "            x1 = float(txt_line[0])\n",
    "            y1 = float(txt_line[1])\n",
    "            x2 = float(txt_line[2])\n",
    "            y2 = float(txt_line[3])\n",
    "            x = (x1 + x2) / 2.\n",
    "            y = (y1 + y2) / 2.\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            x = x / w\n",
    "            y = y / h\n",
    "            width = width / w\n",
    "            height = height / h\n",
    "            contents +=  '0 ' + str(x) + ' ' + str(y) + ' ' + str(width) + ' ' + str(height) + '\\n'\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(txt_line)\n",
    "\n",
    "    if train:\n",
    "        file = open('/usr/src/app/CrowdCounting-P2PNet/yolo_NWPU/labels/train/' + new_txt_file, 'w')\n",
    "    else:\n",
    "        file = open('/usr/src/app/CrowdCounting-P2PNet/yolo_NWPU/labels/val/' + new_txt_file, 'w')\n",
    "\n",
    "    file.writelines(contents)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "test_path = '/usr/src/app/CrowdCounting-P2PNet/yolo_NWPU/images/train/0019.jpg'\n",
    "test_txt = '/usr/src/app/CrowdCounting-P2PNet/yolo_NWPU/labels/train/0019.txt'\n",
    "\n",
    "a = cv2.imread(test_path)\n",
    "f = open(test_txt, 'r')\n",
    "b = f.read().split('\\n')\n",
    "height, width, channel = a.shape\n",
    "\n",
    "for idx in range(len(b)):\n",
    "    d = b[idx].split(' ')\n",
    "    if len(d) < 3:\n",
    "        break\n",
    "    x, y, w, h = d[0], d[1], d[2], d[3]\n",
    "    x = float(x)\n",
    "    y = float(y)\n",
    "    w = float(w)\n",
    "    h = float(h)\n",
    "    x1 = x - w / 2.\n",
    "    y1 = y - h / 2.\n",
    "    x2 = x + w / 2.\n",
    "    y2 = y + h / 2.\n",
    "    \n",
    "    a = cv2.rectangle(a, (int(x1*width), int(y1*height)), (int(x2*width), int(y2*height)), (0, 0, 255), 2)\n",
    "\n",
    "cv2.imwrite('./test.jpg', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.15235690235690236',\n",
       " '0.9653130287648054',\n",
       " '0.02499999999999999',\n",
       " '0.01598984771573599']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(find_extlist('/usr/src/app/CrowdCounting-P2PNet/yolo_NWPU/images/val', '.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(find_extlist('/usr/src/app/CrowdCounting-P2PNet/yolo_NWPU/labels/val', '.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2888"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(find_extlist('/usr/src/app/CrowdCounting-P2PNet/yolo_NWPU/images/train', '.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2888"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(find_extlist('/usr/src/app/CrowdCounting-P2PNet/yolo_NWPU/labels/train', '.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "z = np.zeros((4, 4))\n",
    "n = np.arange(1, 5)\n",
    "n = n.reshape(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[1:3, 1:3] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 1., 2., 0.],\n",
       "       [0., 3., 4., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
